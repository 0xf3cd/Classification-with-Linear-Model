{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import datasets, model_selection, preprocessing, decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_dataset = datasets.load_breast_cancer()\n",
    "# cancer_data = pd.DataFrame(data=cancer_dataset.data, columns=cancer_dataset.feature_names)\n",
    "\n",
    "# label all items in the dataset\n",
    "# cancer_data['type'] = [cancer_dataset.target_names[idx] for idx in cancer_dataset.target]\n",
    "# cancer_data.head() # the last column \"type\" was added\n",
    "\n",
    "cancer_X = cancer_dataset.data\n",
    "cancer_y = cancer_dataset.target\n",
    "\n",
    "# cancer_X_recip = cancer_X.copy()\n",
    "# m, n = cancer_X_recip.shape\n",
    "# for i in range(m):\n",
    "#     for j in range(n):\n",
    "#         value = cancer_X_recip[i][j]\n",
    "#         cancer_X_recip[i][j] = (1.0 / value) if value != 0 else 0.\n",
    "\n",
    "# cancer_X = np.hstack((cancer_X, cancer_X_recip))\n",
    "\n",
    "X_y_dict['cancer'] = [cancer_X, cancer_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_dataset = datasets.load_iris()\n",
    "# iris_data = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
    "\n",
    "# label all items in the dataset\n",
    "# iris_data['type'] = [iris_dataset.target_names[idx] for idx in iris_dataset.target]\n",
    "# iris_data.head() # the last column \"type\" was added\n",
    "\n",
    "iris_X = iris_dataset.data \n",
    "iris_y = iris_dataset.target\n",
    "\n",
    "# iris_X_recip = iris_X.copy()\n",
    "# m, n = iris_X_recip.shape\n",
    "# for i in range(m):\n",
    "#     for j in range(n):\n",
    "#         value = iris_X_recip[i][j]\n",
    "#         iris_X_recip[i][j] = (1.0 / value) if value != 0 else 0.\n",
    "\n",
    "# iris_X = np.hstack((iris_X, np.power(iris_X, 2), iris_X_recip, np.power(iris_X_recip, 2)))\n",
    "\n",
    "        \n",
    "X_y_dict['iris'] = [iris_X, iris_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = np.loadtxt('testdata.csv')\n",
    "testdata_X = testdata[:, 0:-1]\n",
    "testdata_y = testdata[:, -1]\n",
    "\n",
    "X_y_dict['testdata'] = [testdata_X, testdata_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1055,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heart_df = pd.read_csv('heart.csv')\n",
    "\n",
    "# first = pd.get_dummies(heart_df['cp'], prefix=\"cp\")\n",
    "# second = pd.get_dummies(heart_df['slope'], prefix=\"slope\")\n",
    "# thrid = pd.get_dummies(heart_df['thal'], prefix=\"thal\")\n",
    "\n",
    "# heart_df = heart_df.drop(columns = ['cp', 'slope', 'thal'])\n",
    "# heart_df = pd.concat([first, second, thrid, heart_df], axis=1)\n",
    "\n",
    "# heart_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_df = pd.read_csv('heart.csv')\n",
    "\n",
    "features_to_norm = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'ca']\n",
    "\n",
    "cp = pd.get_dummies(heart_df['cp'], prefix=\"cp\")\n",
    "slope = pd.get_dummies(heart_df['slope'], prefix=\"slope\")\n",
    "thal = pd.get_dummies(heart_df['thal'], prefix=\"thal\")\n",
    "heart_df = heart_df.drop(columns = ['cp', 'slope', 'thal'])\n",
    "\n",
    "heart_df_p1 = heart_df[features_to_norm]\n",
    "heart_df_p2 = heart_df.drop(columns = features_to_norm)\n",
    "# print(heart_df_p1.loc[0])\n",
    "\n",
    "heart_X_p1 = heart_df_p1.to_numpy()\n",
    "\n",
    "heart_X_p1_recip = heart_X_p1.copy()\n",
    "m, n = heart_X_p1_recip.shape\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        value = heart_X_p1_recip[i][j]\n",
    "        heart_X_p1_recip[i][j] = (1.0 / value) if value != 0 else 0.\n",
    "#         heart_X_p1_recip[i][j] = sigmoid(value)\n",
    "# print(heart_X_p1_recip[0])\n",
    "\n",
    "# heart_X_p1 = np.hstack((heart_X_p1, np.power(heart_X_p1, 2)))\n",
    "# heart_X_p1 = np.hstack((heart_X_p1, np.power(heart_X_p1, 2), heart_X_p1_recip))\n",
    "heart_X_p1 = np.hstack((heart_X_p1, heart_X_p1_recip))\n",
    "# heart_X_p1 = np.hstack((heart_X_p1, np.power(heart_X_p1, 2), heart_X_p1_recip, np.power(heart_X_p1_recip, 2)))\n",
    "# heart_X_p1 = np.hstack((heart_X_p1, np.power(heart_X_p1, 2), np.power(heart_X_p1, 3), heart_X_p1_recip))\n",
    "# print(heart_X_p1_recip[0])\n",
    "\n",
    "ss = preprocessing.StandardScaler()\n",
    "heart_X_p1 = ss.fit_transform(heart_X_p1)\n",
    "# print(heart_X_p1[0])\n",
    "\n",
    "heart_df_p2 = pd.concat([cp, slope, thal, heart_df_p2], axis=1)\n",
    "heart_X_p2 = heart_df_p2.to_numpy()[:, 0:-1]\n",
    "heart_y = heart_df_p2.to_numpy()[:, -1]\n",
    "# print(heart_X_p2[0])\n",
    "\n",
    "\n",
    "heart_X = np.hstack((heart_X_p1, heart_X_p2))\n",
    "\n",
    "PCA = decomposition.PCA(10)\n",
    "new_heart_X = PCA.fit_transform(heart_X)\n",
    "\n",
    "X_y_dict['heart'] = [new_heart_X, heart_y]\n",
    "# print(heart_df.loc[0])\n",
    "# print(heart_X[0])\n",
    "# print(new_heart_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1066,
   "metadata": {},
   "outputs": [],
   "source": [
    "[X, y] = X_y_dict['cancer']\n",
    "\n",
    "\n",
    "feature_amount = X.shape[1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.25)\n",
    "\n",
    "# features_to_norm = ['age', 'trestbps', 'chol', 'tahlach', 'oldpeak', 'ca']\n",
    "# ss = preprocessing.StandardScaler()\n",
    "# X_train[features_to_norm] = ss.fit_transform(X_train[features_to_norm])\n",
    "# X_test[features_to_norm] = ss.transform(X_test[features_to_norm])\n",
    "\n",
    "train_ones = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.hstack((X_train, train_ones))\n",
    "\n",
    "test_ones = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.hstack((X_test, test_ones))\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0], 1))\n",
    "y_test = y_test.reshape((y_test.shape[0], 1))\n",
    "\n",
    "# print(X_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)\n",
    "# print(feature_amount)\n",
    "# print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1067,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression模型训练集的准确率：0.972\n",
      "Logistic Regression模型测试集的准确率：0.944\n",
      "Logistic Regression模型正确率：0.944\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2', solver='newton-cg', multi_class='auto', max_iter=2000)\n",
    "lr.fit(X_train,y_train.ravel())\n",
    "\n",
    "print(\"Logistic Regression模型训练集的准确率：%.3f\" %lr.score(X_train, y_train.ravel()))\n",
    "print(\"Logistic Regression模型测试集的准确率：%.3f\" %lr.score(X_test, y_test.ravel()))\n",
    "from sklearn import metrics\n",
    "y_hat = lr.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test.ravel(), y_hat) #错误率，也就是np.average(y_test==y_pred)\n",
    "print(\"Logistic Regression模型正确率：%.3f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "def predict_prob(theta, X):\n",
    "    z = X.dot(theta)\n",
    "    return sigmoid(z)\n",
    "\n",
    "def predict(y_pred_prob):\n",
    "    y_pred = [([1.] if item >= 0.5 else [0.]) for item in y_pred_prob]\n",
    "    return np.array(y_pred)\n",
    "\n",
    "def get_cost_value(y, y_pred_prob):\n",
    "    sample_amount = y.shape[0]\n",
    "    value_sum = 0\n",
    "    for i in range(sample_amount):\n",
    "        value_sum += y[i] * np.log(y_pred_prob[i]) \\\n",
    "                     + (1 - y[i]) * np.log(1 - y_pred_prob[i])\n",
    "    return (-1.0 / sample_amount) * value_sum[0] \n",
    "\n",
    "def get_correct_pred_num(y, y_pred):\n",
    "    count = 0\n",
    "    for i in range(y_pred.shape[0]):\n",
    "        if y_pred[i][0] == y[i][0]:\n",
    "            count += 1 \n",
    "    return count\n",
    "\n",
    "def get_accuracy(y, y_pred_prob):\n",
    "    count = get_correct_pred_num(y, y_pred_prob)\n",
    "    return count / y.shape[0]\n",
    "\n",
    "def update_BGD(theta, X, y, y_pred_prob, alpha=0.005):\n",
    "    sample_amount = X.shape[0]\n",
    "    dtheta = X.T.dot(y - y_pred_prob) / sample_amount\n",
    "    theta = theta + alpha * dtheta\n",
    "    return theta \n",
    "\n",
    "def update_SGD(theta, X, y, y_pred_prob, alpha=0.005):\n",
    "    for i in range(X.shape[0]):\n",
    "        dtheta = X[i] * (y[i] - y_pred_prob[i])\n",
    "        for j in range(feature_amount+1):\n",
    "            theta[j][0] += alpha * dtheta[j]\n",
    "    return theta\n",
    "    \n",
    "def train(theta, X, y, iterations=1000):\n",
    "    cost_value_record = []\n",
    "    accuracy_record = []\n",
    "    theta_record = []\n",
    "    update_func = update_SGD\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        y_pred_prob = predict_prob(theta, X)\n",
    "        cost_value = get_cost_value(y, y_pred_prob)\n",
    "        accuracy = get_accuracy(y, predict(y_pred_prob))\n",
    "        # print(i, accuracy, cost_value)\n",
    "\n",
    "        theta = update_func(theta, X, y, y_pred_prob)\n",
    "                \n",
    "        cost_value_record.append(cost_value)\n",
    "        accuracy_record.append(accuracy)\n",
    "        theta_record.append(theta.copy())\n",
    "        \n",
    "        if i % 200 == 0:\n",
    "            y_test_pred_prob = predict_prob(theta, X_test)\n",
    "            y_test_pred = predict(y_test_pred_prob)\n",
    "            print('iteration', i, 'finished. Acc: ', accuracy, get_accuracy(y_test, y_test_pred))\n",
    "\n",
    "    \n",
    "    return theta, cost_value_record, accuracy_record, theta_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 finished. Acc:  0.6123348017621145 0.6578947368421053\n",
      "iteration 200 finished. Acc:  0.8414096916299559 0.8026315789473685\n",
      "iteration 400 finished. Acc:  0.8546255506607929 0.8026315789473685\n",
      "iteration 600 finished. Acc:  0.8281938325991189 0.7368421052631579\n",
      "iteration 800 finished. Acc:  0.8370044052863436 0.8157894736842105\n",
      "iteration 1000 finished. Acc:  0.8237885462555066 0.7368421052631579\n",
      "iteration 1200 finished. Acc:  0.8237885462555066 0.7368421052631579\n",
      "iteration 1400 finished. Acc:  0.8281938325991189 0.7368421052631579\n",
      "iteration 1600 finished. Acc:  0.8458149779735683 0.7763157894736842\n",
      "iteration 1800 finished. Acc:  0.762114537444934 0.7894736842105263\n",
      "iteration 2000 finished. Acc:  0.8546255506607929 0.7894736842105263\n",
      "iteration 2200 finished. Acc:  0.8370044052863436 0.8421052631578947\n",
      "iteration 2400 finished. Acc:  0.7665198237885462 0.7894736842105263\n",
      "iteration 2600 finished. Acc:  0.7577092511013216 0.7894736842105263\n",
      "iteration 2800 finished. Acc:  0.8414096916299559 0.8026315789473685\n",
      "iteration 3000 finished. Acc:  0.8325991189427313 0.8026315789473685\n",
      "iteration 3200 finished. Acc:  0.8325991189427313 0.8026315789473685\n",
      "iteration 3400 finished. Acc:  0.8325991189427313 0.8026315789473685\n",
      "iteration 3600 finished. Acc:  0.8193832599118943 0.8026315789473685\n",
      "iteration 3800 finished. Acc:  0.7577092511013216 0.7894736842105263\n",
      "iteration 4000 finished. Acc:  0.7709251101321586 0.7894736842105263\n",
      "iteration 4200 finished. Acc:  0.7665198237885462 0.7894736842105263\n",
      "iteration 4400 finished. Acc:  0.762114537444934 0.7894736842105263\n",
      "iteration 4600 finished. Acc:  0.7709251101321586 0.7894736842105263\n",
      "iteration 4800 finished. Acc:  0.762114537444934 0.7894736842105263\n",
      "54 91 0.7105263157894737\n"
     ]
    }
   ],
   "source": [
    "init_theta = np.random.randn(feature_amount+1, 1)\n",
    "# print(init_theta)\n",
    "\n",
    "final_theta, cost_values, accs, thetas = train(init_theta.copy(), X_train, y_train, 5000)\n",
    "# print(cost_values[-1], accs[-1])\n",
    "\n",
    "y_test_pred_prob = predict_prob(final_theta, X_test)\n",
    "y_test_pred = predict(y_test_pred_prob)\n",
    "# print(y_test_pred)\n",
    "# print(y_test)\n",
    "\n",
    "count = get_correct_pred_num(y_test, y_test_pred)\n",
    "print(count, y_pred.shape[0], get_accuracy(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=2000, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='newton-cg',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 710,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty='l2', solver='newton-cg', multi_class='auto', max_iter=2000)\n",
    "lr.fit(X_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression模型训练集的准确率：0.850\n",
      "Logistic Regression模型测试集的准确率：0.868\n",
      "Logistic Regression模型正确率：0.868\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression模型训练集的准确率：%.3f\" %lr.score(X_train, y_train.ravel()))\n",
    "print(\"Logistic Regression模型测试集的准确率：%.3f\" %lr.score(X_test, y_test.ravel()))\n",
    "from sklearn import metrics\n",
    "y_hat = lr.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test.ravel(), y_hat) #错误率，也就是np.average(y_test==y_pred)\n",
    "print(\"Logistic Regression模型正确率：%.3f\" %accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
